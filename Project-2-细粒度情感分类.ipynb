{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hint(string): return print(''.join(map(chr, map(lambda x: int(x, 16), string.split('.')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分 背景与问题描述"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "+ jupyter notebook (测试、试验环境)\n",
    "+ Pycharm （开发环境）\n",
    "+ python3.6\n",
    "+ networkx\n",
    "+ jieba\n",
    "+ numpy, pandas, matplotlib\n",
    "+ gensim\n",
    "+ (optional) bottle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在自然语言处理中，有一个常见的问题就是对客户的评价进行分析。 这些用户评论中，包含了大量的有用信息，例如情感分析，或者相关事实描述。 例如，\n",
    "> `“味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“`\n",
    "\n",
    "首先情感是正向的，除此之外我们还能够进行知道这个的几个事实描述：1. 性价比比较高； 2. 装修比较好； 3. 分量足。 \n",
    "\n",
    "这些信息是非常重要宝贵的，不论是对于公司进行商业分析或者要建立一个搜索引擎排序，这些信息都是重要的参考因素。 那么在这个时候，我们就需要进行文本的情感分类了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注**: 此次问题的来源数据集来源于大众点评， 这个数据集也被用作 AI 全球挑战赛的数据集。 而且细粒度情感分类这个问题其实在目前而言是一个 *未解之谜*， 人类现在对这个问题并没有什么特别好的方法， 因为人的情感表达真的是很有变化的，例如“我不认为这个地方不好是不对的”。所以这个问题也需要大家在之后做出来基础模型之后，大家多想想办法，*八仙过海，各显神通*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，我们这一次是要使用深度学习的方法，建立一个模型，这个模型能够将一句话进行分类判断，判断出来这句话到底表达了什么重要信息。 说实话，这个看似简单的问题，曾经是困扰了科学家数十年的问题，就算是现在，深度学习，人工智能有了很大的进步，其效果也达不到人们预期的那么好,但是比起前些年已经好多了:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个问题我们希望的是，输入一句话，输出是这句话对于以下6大类，20小类进行打标，对于每个小类而言，都会有<** 正面情感, 中性情感, 负面情感, 情感倾向未提及 > ** 这4个类别。 \n",
    "\n",
    "总得来说，我们现在这6大类，20小类的类别如下：\n",
    "\n",
    "+ 位置: location\n",
    "    + 交通是否便利(traffic convenience)\n",
    "    + 距离商圈远近(distance from business district)\n",
    "    + 是否容易寻找(easy to find)\n",
    "+ 服务(service)\t\n",
    "    + 排队等候时间(wait time)\n",
    "    + 服务人员态度(waiter’s attitude)\n",
    "    + 是否容易停车(parking convenience)\n",
    "    + 点菜/上菜速度(serving speed)\n",
    "+ 价格(price)\t\n",
    "    + 价格水平(price level)\n",
    "    + 性价比(cost-effective)\n",
    "    + 折扣力度(discount)\n",
    "+ 环境(environment)\t\n",
    "    + 装修情况(decoration)\n",
    "    + 嘈杂情况(noise)\n",
    "    + 就餐空间(space)\n",
    "    + 卫生情况(cleaness)\n",
    "+ 菜品(dish)\t\n",
    "    + 分量(portion)\n",
    "    + 口感(taste)\n",
    "    + 外观(look)\n",
    "    + 推荐程度(recommendation)\n",
    "+ 其他(others)\t\n",
    "    + 本次消费感受(overall experience)\n",
    "    + 再次消费的意愿(willing to consume again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而为了方便训练数据的标标注，训练数据中，<** 正面情感, 中性情感, 负面情感, 情感倾向未提及 > ** 分别对应与 (1, 0, -1, -2). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例如说，\n",
    "> `“味道不错的面馆，性价比也相当之高，分量很足～女生吃小份，胃口小的，可能吃不完呢。环境在面馆来说算是好的，至少看上去堂子很亮，也比较干净，一般苍蝇馆子还是比不上这个卫生状况的。中午饭点的时候，人很多，人行道上也是要坐满的，隔壁的冒菜馆子，据说是一家，有时候也会开放出来坐吃面的人。“`\n",
    "\n",
    "这句话在训练数据中的标签就是：\n",
    "\n",
    "+ 交通是否便利(traffic convenience)\t-2 \n",
    "+ 距离商圈远近(distance from business district)\t-2\n",
    "+ 是否容易寻找(easy to find)\t-2\n",
    "+ 排队等候时间(wait time)\t-2\n",
    "+ 服务人员态度(waiter’s attitude)\t-2\n",
    "+ 是否容易停车(parking convenience)\t-2\n",
    "+ 点菜/上菜速度(serving speed)\t-2\n",
    "+ 价格水平(price level)\t-2\n",
    "+ 性价比(cost-effective)\t1\n",
    "+ 折扣力度(discount)\t-2\n",
    "+ 装修情况(decoration)\t1\n",
    "+ 嘈杂情况(noise)\t-2\n",
    "+ 就餐空间(space)\t-2\n",
    "+ 卫生情况(cleaness)\t1\n",
    "+ 分量(portion)\t1\n",
    "+ 口感(taste)\t1\n",
    "+ 外观(look)\t-2\n",
    "+ 推荐程度(recommendation)\t-2\n",
    "+ 次消费感受(overall experience)\t1\n",
    "+ 再次消费的意愿(willing to consume again)\t-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集下载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集的下载在 https://challenger.ai/competition/fsauor2018， 大家下载数据集， 以及测试集，注意，训练集我们需要分成 training data, validation data 然后 test data里边的数据绝对不能在训练的时候用。 否则的话就是去了意义。 \n",
    "\n",
    "**注**, 这个数据集之所以被用到了 AI 挑战赛中，是因为其难度很大。 绝大数人在公司中遇到的问题难度**不会**超过这个问题。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评价\n",
    "\n",
    "参照 https://challenger.ai/competition/fsauor2018, 这个问题的评价其实就是多个分类的f1 score 的平均值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: 机器学习中的Loss函数的作用为何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这个答案可以写很多，但是主要是要涉及到：Loss 函数用来衡量机器学习过程中模型表现的，我们通过 Loss 函数来进行优化模型或者选择模型\n"
     ]
    }
   ],
   "source": [
    "# remove the # before hint, what you find? \n",
    "hint('8fd9.4e2a.7b54.6848.53ef.4ee5.5199.5f88.591a.ff0c.4f46.662f.4e3b.8981.662f.8981.6d89.53ca.5230.ff1a.4c.6f.73.73.20.51fd.6570.7528.6765.8861.91cf.673a.5668.5b66.4e60.8fc7.7a0b.4e2d.6a21.578b.8868.73b0.7684.ff0c.6211.4eec.901a.8fc7.20.4c.6f.73.73.20.51fd.6570.6765.8fdb.884c.4f18.5316.6a21.578b.6216.8005.9009.62e9.6a21.578b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Q2: 为什么 SVM 适合核函数的方法？（考虑基于拉格朗日距离的 SVM 的 Loss 函数）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "详情请参考我们的课程视频，一个主要的点是，最后证明 SVM 模型的性能只与 x_ix_j 的乘机相关，所以我们可以方便的把x_i x_j 映射到某个新函数上，不改变其 Loss 的单调性即可\n"
     ]
    }
   ],
   "source": [
    "hint('8be6.60c5.8bf7.53c2.8003.6211.4eec.7684.8bfe.7a0b.89c6.9891.ff0c.4e00.4e2a.4e3b.8981.7684.70b9.662f.ff0c.6700.540e.8bc1.660e.20.53.56.4d.20.6a21.578b.7684.6027.80fd.53ea.4e0e.20.78.5f.69.78.5f.6a.20.7684.4e58.673a.76f8.5173.ff0c.6240.4ee5.6211.4eec.53ef.4ee5.65b9.4fbf.7684.628a.78.5f.69.20.78.5f.6a.20.6620.5c04.5230.67d0.4e2a.65b0.51fd.6570.4e0a.ff0c.4e0d.6539.53d8.5176.20.4c.6f.73.73.20.7684.5355.8c03.6027.5373.53ef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: 决策树的 Loss 函数是什么？随机森林是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一、熵的和最小，要选择一个条件让这次分类的两边结果竟可能的‘纯’, 例如，我们有[2, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "如果我们选择一个条件是'是不是1'，我们可以分成[0, 0, 0, 0, 2], [1, 1, 1, 1, 1]， 也可以条件是'是不是2', \n",
      "我们就可以分成[2], [1, 0, 0, 0, 0, 1, 1, 1, 1, 1]， 显然前者更纯。 如果同学们忘了熵的概念，大家赶紧再查一下，然后计算一下这两个分类结果的熵是多大；\n",
      "二、随机森林是用很多小的决策树用来投票的集成模型（Ensemble），每个小决策树使用一部分的 feature 进行训练\n"
     ]
    }
   ],
   "source": [
    "hint(\"\"\"4e00.3001.71b5.7684.548c.6700.5c0f.ff0c.8981.9009.62e9.4e00.4e2a.6761.4ef6.8ba9.8fd9.6b21.5206.7c7b.7684.4e24.8fb9.7ed3.679c.7adf.53ef.80fd.7684.2018.7eaf.2019.2c.20.4f8b.5982.ff0c.6211.4eec.6709.5b.32.2c.20.31.2c.20.30.2c.20.30.2c.20.30.2c.20.30.2c.20.31.2c.20.31.2c.20.31.2c.20.31.2c.20.31.5d.a.5982.679c.6211.4eec.9009.62e9.4e00.4e2a.6761.4ef6.662f.27.662f.4e0d.662f.31.27.ff0c.6211.4eec.53ef.4ee5.5206.6210.5b.30.2c.20.30.2c.20.30.2c.20.30.2c.20.32.5d.2c.20.5b.31.2c.20.31.2c.20.31.2c.20.31.2c.20.31.5d.ff0c.20.4e5f.53ef.4ee5.6761.4ef6.662f.27.662f.4e0d.662f.32.27.2c.20.a.6211.4eec.5c31.53ef.4ee5.5206.6210.5b.32.5d.2c.20.5b.31.2c.20.30.2c.20.30.2c.20.30.2c.20.30.2c.20.31.2c.20.31.2c.20.31.2c.20.31.2c.20.31.5d.ff0c.20.663e.7136.524d.8005.66f4.7eaf.3002.20.5982.679c.540c.5b66.4eec.5fd8.4e86.71b5.7684.6982.5ff5.ff0c.5927.5bb6.8d76.7d27.518d.67e5.4e00.4e0b.ff0c.7136.540e.8ba1.7b97.4e00.4e0b.8fd9.4e24.4e2a.5206.7c7b.7ed3.679c.7684.71b5.662f.591a.5927.ff1b.a.4e8c.3001.968f.673a.68ee.6797.662f.7528.5f88.591a.5c0f.7684.51b3.7b56.6811.7528.6765.6295.7968.7684.96c6.6210.6a21.578b.ff08.45.6e.73.65.6d.62.6c.65.ff09.ff0c.6bcf.4e2a.5c0f.51b3.7b56.6811.4f7f.7528.4e00.90e8.5206.7684.20.66.65.61.74.75.72.65.20.8fdb.884c.8bad.7ec3\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**插入**：我们在这里快速的过一下如何在 Jupyter 中写数学公式。 这个其实很简单，如果我们要输入一个公式，例如A1, 那么，我们在Jupyter中输入`$A_i$`, 然后 Enter， 是不是就变成了$A_i$? 其实两个`$$`之间的东西就是 Latex 的符号，`$..$`这个我们叫做inline模式，意思就是说你写出来的公式是和你的文字在一行里，如果你`$$..$$``，这个公式就会单独是一行。\n",
    "\n",
    "我们现在再试一个, 输入`$$\\frac{P_i}{\\sum_{j \\in \\mathbf{V}}^NP_j}$$`, 输完之后 Enter， 你看到了什么？ \n",
    "\n",
    "$$\\frac{P_i}{\\sum_{j \\in \\mathbf{V}}^NP_j}$$\n",
    "\n",
    "这个时候会有同学说，可是这些符号，我怎么记得住呢？ 我给大家提供了一个参考手册，大家有空就看看 https://github.com/Artificial-Intelligence-for-NLP/comment-setimental-classification/blob/master/Latex-Symbols.pdf，熟能生巧。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: 使用Latex 写出来决策树希望找到一个 feature，这个 feature 使得熵的和最少的公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5: 贝叶斯公式的原理是什么？ 我们现在用的贝叶斯分类器为什么是“朴素贝叶斯”， 它为什么朴素？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6: 神经网络的Loss函数的作用为何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "神经网络里面的Loss函数是用来衡量模型的好坏，Loss函数越大，预测与实际的误差越大，预测越不准确。为了让预测结果更加精准，我们要减少Loss函数，通过梯度下降法，利用反向传播不停迭代调整神经网络中的参数，找到使Loss函数最小的参数，确定模型。\n"
     ]
    }
   ],
   "source": [
    "hint(\"\"\"795e.7ecf.7f51.7edc.91cc.9762.7684.4c.6f.73.73.51fd.6570.662f.7528.6765.8861.91cf.6a21.578b.7684.597d.574f.ff0c.4c.6f.73.73.51fd.6570.8d8a.5927.ff0c.9884.6d4b.4e0e.5b9e.9645.7684.8bef.5dee.8d8a.5927.ff0c.9884.6d4b.8d8a.4e0d.51c6.786e.3002.4e3a.4e86.8ba9.9884.6d4b.7ed3.679c.66f4.52a0.7cbe.51c6.ff0c.6211.4eec.8981.51cf.5c11.4c.6f.73.73.51fd.6570.ff0c.901a.8fc7.68af.5ea6.4e0b.964d.6cd5.ff0c.5229.7528.53cd.5411.4f20.64ad.4e0d.505c.8fed.4ee3.8c03.6574.795e.7ecf.7f51.7edc.4e2d.7684.53c2.6570.ff0c.627e.5230.4f7f.4c.6f.73.73.51fd.6570.6700.5c0f.7684.53c2.6570.ff0c.786e.5b9a.6a21.578b.3002\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7: 神经网络的激活函数(activation function)起什么作用？ 如果没有激活函数会怎么样？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "激活函数用来进行非线性变化，不断得非线性变化使得我们(理论上)可以拟合任意函数，这也是为什么神经网络能`学习`的原因。神经网络里边的`学习`其实就是函数拟合的意思\n"
     ]
    }
   ],
   "source": [
    "hint('6fc0.6d3b.51fd.6570.7528.6765.8fdb.884c.975e.7ebf.6027.53d8.5316.ff0c.4e0d.65ad.5f97.975e.7ebf.6027.53d8.5316.4f7f.5f97.6211.4eec.28.7406.8bba.4e0a.29.53ef.4ee5.62df.5408.4efb.610f.51fd.6570.ff0c.8fd9.4e5f.662f.4e3a.4ec0.4e48.795e.7ecf.7f51.7edc.80fd.60.5b66.4e60.60.7684.539f.56e0.3002.795e.7ecf.7f51.7edc.91cc.8fb9.7684.60.5b66.4e60.60.5176.5b9e.5c31.662f.51fd.6570.62df.5408.7684.610f.601d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8: 神经网络的softmax如何理解， 其作用是什么？ 在`答案`中写出softmax的python表达；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q9: 简述 normalized_1 和softmax函数的相同点和不同点， 说明softmax相比normalized_1该函数的优势所在"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "output = np.array([y1, y2, y3])\n",
    "\n",
    "normalized_1 = output / np.sum(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q10: 写出crossentropy的函数表达式，说明该函数的作用和意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------- 休息一下，接下来是关于 Word2Vec的 ------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11: 说明word2vec要解决的问题背景， 以及word2vec的基本思路， 说明word2vec比起之前方法的优势；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12: 说明word2vec的预测目标， predication target, 在答案中写出skip-gram和cbow的预测概率；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q13: 请说明word2vec的两种常见优化方法，分别阐述其原理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q14: 请说明word2vec中哈夫曼树的作用；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q15: 哈夫曼树如何构建？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. https://github.com/heineman/python-data-structures/blob/master/5.%20Heap-based%20Structures/huffman.py\n",
      "2. https://github.com/RaRe-Technologies/gensim/blob/3d5a21c1c8128cb8dd4f6e51e9ef3dc5af000871/gensim/models/deprecated/word2vec.py#L670\"\n",
      "3. https://www.wikiwand.com/en/Huffman_coding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hint('a.31.2e.20.68.74.74.70.73.3a.2f.2f.67.69.74.68.75.62.2e.63.6f.6d.2f.68.65.69.6e.65.6d.61.6e.2f.70.79.74.68.6f.6e.2d.64.61.74.61.2d.73.74.72.75.63.74.75.72.65.73.2f.62.6c.6f.62.2f.6d.61.73.74.65.72.2f.35.2e.25.32.30.48.65.61.70.2d.62.61.73.65.64.25.32.30.53.74.72.75.63.74.75.72.65.73.2f.68.75.66.66.6d.61.6e.2e.70.79.a.32.2e.20.68.74.74.70.73.3a.2f.2f.67.69.74.68.75.62.2e.63.6f.6d.2f.52.61.52.65.2d.54.65.63.68.6e.6f.6c.6f.67.69.65.73.2f.67.65.6e.73.69.6d.2f.62.6c.6f.62.2f.33.64.35.61.32.31.63.31.63.38.31.32.38.63.62.38.64.64.34.66.36.65.35.31.65.39.65.66.33.64.63.35.61.66.30.30.30.38.37.31.2f.67.65.6e.73.69.6d.2f.6d.6f.64.65.6c.73.2f.64.65.70.72.65.63.61.74.65.64.2f.77.6f.72.64.32.76.65.63.2e.70.79.23.4c.36.37.30.22.a.33.2e.20.68.74.74.70.73.3a.2f.2f.77.77.77.2e.77.69.6b.69.77.61.6e.64.2e.63.6f.6d.2f.65.6e.2f.48.75.66.66.6d.61.6e.5f.63.6f.64.69.6e.67.a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q16: 在gensim中如何实现词向量？ 请将gensim中实现词向量的代码置于答案中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q17: 请说出除了 skip-gram和cbow的其他4中词向量方法的名字， 并且选取其中两个叙述其基本原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Onehot, Glove,Cove,EMLo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hint('a.4f.6e.65.68.6f.74.2c.20.47.6c.6f.76.65.2c.43.6f.76.65.2c.45.4d.4c.6f.a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------- 休息一下，接下来是关于 Keras 和 Tensorflow 使用的 -------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大家先熟悉一下什么是MNIST数据集： \n",
    "\n",
    "> http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    ">https://en.wikipedia.org/wiki/MNIST_database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q18: 参考keras参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks) 实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键代码: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q19:参考tensorflow的参考手册，构建一个机器学习模型，该模型能够完成使用DNN(deep neural networks)实现MNIST数据集的分类；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hints:tensorflow实现MNIST https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/2_fullyconnected.ipynb\n"
     ]
    }
   ],
   "source": [
    "hint('68.69.6e.74.73.3a.74.65.6e.73.6f.72.66.6c.6f.77.5b9e.73b0.4d.4e.49.53.54.20.68.74.74.70.73.3a.2f.2f.67.69.74.68.75.62.2e.63.6f.6d.2f.74.65.6e.73.6f.72.66.6c.6f.77.2f.74.65.6e.73.6f.72.66.6c.6f.77.2f.62.6c.6f.62.2f.6d.61.73.74.65.72.2f.74.65.6e.73.6f.72.66.6c.6f.77.2f.65.78.61.6d.70.6c.65.73.2f.75.64.61.63.69.74.79.2f.32.5f.66.75.6c.6c.79.63.6f.6e.6e.65.63.74.65.64.2e.69.70.79.6e.62')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q20: 参考keras和tensorflow对同一问题的实现，说明keras和tensorflow的异同；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q21: tensorflow 使用 Graph 计算机制的优缺点是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q22: Q18， Q19 的tensorflow 或 keras 模型的训练时准确率和测试集准确率分别是多少？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Q23: 训练时准确率大于测试集准确率的现象叫什么名字，在神经网络中如何解决该问题？(至少提出5个解决方法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Q24: 请使用自己的语言简述通过正则化 (regularization)减小过拟合的原理；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Q25: 在tensorflow官方实例中给出的fully connected 神经网络的分类模型中，数据进行了哪些预处理，这些预处理的原因是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------- 休息一下，接下来是关于 RNN 和 CNN 的 ---------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q26: 简述CNN的原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q27: CNN的 Spatial Invariant是什么意思？ 是如何做到的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q28: CNN增加了很多层数，这些层数使用 filter 进行计算。 按说需要拟合的参数变得很多，请问 CNN 是如何解决这个问题的，如何加快速度的？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main points: Pooling, Parameter Sharing\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hint('a.6d.61.69.6e.20.70.6f.69.6e.74.73.3a.20.50.6f.6f.6c.69.6e.67.2c.20.50.61.72.61.6d.65.74.65.72.20.53.68.61.72.69.6e.67.a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q29: CNN中的 Batch Normalization有什么意义？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q30: CNN中的 Pooling 起到什么作用？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q31: CNN中的 Fully Connect起到什么作业？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q32: 深度网络中的权值初始化有什么讲究？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading: 参照 Keras 和 Tensorflow 的示例，手敲使用 keras, tensorflow + CNN 实现MNIST分类的问题：\n",
    "\n",
    "+ https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "+ https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/4_convolutions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把代码手敲一遍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q33: 简述RNN解决的问题所具有的特点；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q34: 写出RNN实现时间或者序列相关的数学实现(见课程slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q35: 简述RNN的两种重要变体的提出原因和基本原理？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q36:  Attentional RNN 以及 Stacked RNN 和 Bi-RNN 分别是什么，其做了什么改动？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading 和 CNN 类似，请在 Keras,Tensorflow中查找如何实现 RNN 模型\n",
    "\n",
    "+ https://github.com/Artificial-Intelligence-for-NLP/References/blob/master/AI%20%26%20Machine%20Learning/Hands.On.TensorFlow.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分： 项目解决过程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码主要在 Pycharm 里边写，jupyter 里边写一个关键步骤就行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q37: 要实现文本分类或情感分类，文本信息需要进行哪些初始化操作？自己手工实现，keras提供的API，tenorflow提供的API，分别是哪些？请提供关键代码置于下边`回答`中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hint('id_to_word, word_to_id, padding, batched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q38 在没有预训练的词向量时候， keras 如何实现embedding操作，即如何依据一个单词的序列获得其向量表示？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 39: 在**有**预先训练的词向量时候，keras和tensorflow又如何实现embeding操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q40：基于上文进行的数据预处理，使用keras和tensorflow如何构建神经网络模型？请提供关键代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 好的 现在开始切入正题 --\n",
    "\n",
    "其实，我们解决实际问题的时候，很少自己从头到尾写一个神经网络模型，我们往往是找一个效果比较好的类似问题的模型，然后在这个问题上改造。 或者我们在去一个公司的时候，接手的工作也往往是改动以前的模型，所以我们解决这个语义分类问题我们也首先是找一个类似的问题，然后参考一个模型进行修改，变成能够解决我们这个问题的模型。\n",
    "\n",
    "我们以上所以的理论知识，都是用来支持我们做修改，能够看懂别人为何要这样写，然后自己要改哪里。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kaggle上的“恶意评价识别”这个项目和我们的这个项目是类似的, 大家请首先在这个的 Kernel 里边找到一个公开代码的示例，然后选择一个自己能够看懂且效果较好的模型进行改造。\n",
    "\n",
    "+ https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "+ https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/kernels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle这个问题和我们的问题类似，但是并不是完全一样， 其中最不一样的其实是我们的期望的结果这里, Kaggle 这里的输出是5个类别，然后类别的是0~1直接的数字来预测是否是这个类别，然后我们的客户评价问题中，打标对20个分类的-2, -1, 0, 1四个标记. 一种最简单的方法，是把这个20分类问题变成80分类问题，然后每个分类的输出是0或者1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体是借鉴的当时的参赛大佬的，当时他的f1_sroce是0.70201. 他的github:  https://github.com/pengshuang/AI-Comp .   \n",
    "他用的是对每个类分别建模，用20个模型， 每个模型训练20epoch，确实取得了不错的效果但是即使是在google colab上运行，一个epoch还是要5分多钟，训练完20个模型花费的时间实在太长。 所以就想在此基础上改成80分类。此外用fasttext的pre-trained的词嵌入（原作者没有用pre-trained）。 看看是否对最终结果有所提升。\n",
    "\n",
    "由于硬件限制，代码都放在googel colab上运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature engine\n",
    "import random\n",
    "\n",
    "random.seed = 16\n",
    "import pandas as pd\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "data = pd.read_csv(\"./input/sentiment_analysis_trainingset.csv\")\n",
    "\n",
    "stopwords = []\n",
    "with open(\"../my_solution/input/stop_words.txt\", encoding='gbk') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        stopwords.append(line)\n",
    "\n",
    "\n",
    "def segWord(doc):\n",
    "    seg_list = jieba.cut(doc, cut_all=False)\n",
    "    return list(seg_list)\n",
    "\n",
    "\n",
    "# move stop words\n",
    "def filter_map(arr):\n",
    "    res = \"\"\n",
    "    for c in arr:\n",
    "        if c not in stopwords and c != ' ' and c != '\\xa0' and c != '\\n' and c != '\\ufeff' and c != '\\r':\n",
    "            res += c\n",
    "    return res\n",
    "\n",
    "\n",
    "# move stop words and generate char sent\n",
    "def filter_char_map(arr):\n",
    "    res = []\n",
    "    for c in arr:\n",
    "        if c not in stopwords and c != ' ' and c != '\\xa0' and c != '\\n' and c != '\\ufeff' and c != '\\r':\n",
    "            res.append(c)\n",
    "    return \" \".join(res)\n",
    "\n",
    "\n",
    "# get char of sentence\n",
    "def get_char(arr):\n",
    "    res = []\n",
    "    for c in arr:\n",
    "        res.append(c)\n",
    "    return list(res)\n",
    "#labels ont-hot encoding, get 80 labels\n",
    "def enconding_labels(datasets):\n",
    "    cut_regions = [-3, -2, -1, 0, 1]\n",
    "    labels = datasets.iloc[:, 2:]\n",
    "    labels_cut_regions = {}\n",
    "\n",
    "    for label in labels:\n",
    "        labels_cut_regions[label] = cut_regions\n",
    "\n",
    "    cut_df = pd.DataFrame()\n",
    "    for field in labels_cut_regions.keys():\n",
    "        cut_series = pd.cut(datasets[field], labels_cut_regions[field], right=True)\n",
    "        onehot_df = pd.get_dummies(cut_series, prefix=field)\n",
    "        cut_df = pd.concat([cut_df, onehot_df], axis=1)\n",
    "    new_df = pd.concat([datasets, cut_df], axis=1)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "data.content = data.content.map(lambda x: filter_map(x))\n",
    "data.content = data.content.map(lambda x: get_char(x))\n",
    "data = enconding_labels(data)\n",
    "data.to_csv(\"preprocess/train_char.csv\", index=None)\n",
    "\n",
    "validation = pd.read_csv(\"./input/sentiment_analysis_validationset.csv\")\n",
    "validation.content = validation.content.map(lambda x: filter_map(x))\n",
    "validation.content = validation.content.map(lambda x: get_char(x))\n",
    "validation = enconding_labels(validation)\n",
    "validation.to_csv(\"preprocess/validation_char.csv\", index=None)\n",
    "\n",
    "test = pd.read_csv(\"./input/sentiment_analysis_testa.csv\")\n",
    "test.content = test.content.map(lambda x: filter_map(x))\n",
    "test.content = test.content.map(lambda x: get_char(x))\n",
    "test = enconding_labels(test)\n",
    "test.to_csv(\"preprocess/test_char.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Join attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints\n",
    "from keras.layers.merge import _Merge\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(Attention())\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        # self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        input_shape = K.int_shape(x)\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        # step_dim = self.step_dim\n",
    "        step_dim = input_shape[1]\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b[:input_shape[1]]\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "    \t# print weigthted_input.shape\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # return input_shape[0], input_shape[-1]\n",
    "    \treturn input_shape[0], self.features_dim\n",
    "# end Attention\n",
    "\n",
    "\n",
    "class JoinAttention(_Merge):\n",
    "    def __init__(self, step_dim, hid_size,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Attention mechanism according to other vector.\n",
    "        Supports Masking.\n",
    "        # Input shape, list of\n",
    "            2D tensor with shape: `(samples, features_1)`.\n",
    "            3D tensor with shape: `(samples, steps, features_2)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            en = LSTM(64, return_sequences=False)(input)\n",
    "            de = LSTM(64, return_sequences=True)(input2)\n",
    "            output = JoinAttention(64, 20)([en, de])\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        # self.init = initializations.get('glorot_uniform')\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.hid_size = hid_size\n",
    "        super(JoinAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if not isinstance(input_shape, list):\n",
    "            raise ValueError('A merge layer [JoinAttention] should be called '\n",
    "                             'on a list of inputs.')\n",
    "        if len(input_shape) != 2:\n",
    "            raise ValueError('A merge layer [JoinAttention] should be called '\n",
    "                             'on a list of 2 inputs. '\n",
    "                             'Got ' + str(len(input_shape)) + ' inputs.')\n",
    "        if len(input_shape[0]) != 2 or len(input_shape[1]) != 3:\n",
    "            raise ValueError('A merge layer [JoinAttention] should be called '\n",
    "                             'on a list of 2 inputs with first ndim 2 and second one ndim 3. '\n",
    "                             'Got ' + str(len(input_shape)) + ' inputs.')\n",
    "\n",
    "        self.W_en1 = self.add_weight((input_shape[0][-1], self.hid_size),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W0'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.W_en2 = self.add_weight((input_shape[1][-1], self.hid_size),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W1'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.W_de = self.add_weight((self.hid_size,),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W2'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "\n",
    "        if self.bias:\n",
    "            self.b_en1 = self.add_weight((self.hid_size,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b0'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "            self.b_en2 = self.add_weight((self.hid_size,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b1'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "            self.b_de = self.add_weight((input_shape[1][1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b2'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b_en1 = None\n",
    "            self.b_en2 = None\n",
    "            self.b_de = None\n",
    "\n",
    "        self._reshape_required = False\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[1][0], input_shape[1][-1]\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        en = inputs[0]\n",
    "        de = inputs[1]\n",
    "        de_shape = K.int_shape(de)\n",
    "        step_dim = de_shape[1]\n",
    "\n",
    "        hid_en = K.dot(en, self.W_en1)\n",
    "        hid_de = K.dot(de, self.W_en2)\n",
    "        if self.bias:\n",
    "            hid_en += self.b_en1\n",
    "            hid_de += self.b_en2\n",
    "        hid = K.tanh(K.expand_dims(hid_en, axis=1) + hid_de)\n",
    "        eij = K.reshape(K.dot(hid, K.reshape(self.W_de, (self.hid_size, 1))), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            eij += self.b_de[:step_dim]\n",
    "\n",
    "        a = K.exp(eij - K.max(eij, axis=-1, keepdims=True))\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask[1], K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = de * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "# end JoinAttention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import Model\n",
    "from keras.layers import *\n",
    "from JoinAttLayer import Attention     #using attention mechanism\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred, beta=1):\n",
    "    \"\"\"Computes the F score.\n",
    "    The F score is the weighted harmonic mean of precision and recall.\n",
    "    Here it is only computed as a batch-wise average, not globally.\n",
    "    This is useful for multi-label classification, where input samples can be\n",
    "    classified as sets of labels. By only using accuracy (precision) a model\n",
    "    would achieve a perfect score by simply assigning every class to every\n",
    "    input. In order to avoid this, a metric should penalize incorrect class\n",
    "    assignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\n",
    "    computes this, as a weighted mean of the proportion of correct class\n",
    "    assignments vs. the proportion of incorrect class assignments.\n",
    "    With beta = 1, this is equivalent to a F-measure. With beta < 1, assigning\n",
    "    correct classes becomes more important, and with beta > 1 the metric is\n",
    "    instead weighted towards penalizing incorrect class assignments.\n",
    "    \"\"\"\n",
    "    if beta < 0:\n",
    "        raise ValueError('The lowest choosable beta is zero (only precision).')\n",
    "\n",
    "    p = precision(y_true, y_pred)\n",
    "    r = recall(y_true, y_pred)\n",
    "    bb = beta ** 2\n",
    "    fbeta_score = (1 + bb) * (p * r) / (bb * p + r + K.epsilon())\n",
    "    return fbeta_score\n",
    "\n",
    "\n",
    "#line 7-53 define three evalution index.\n",
    "\n",
    "\n",
    "\n",
    "class TextClassifier():                                       #model construction.\n",
    "\n",
    "    def model(self, embeddings_matrix, maxlen, word_index, num_class):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        encode = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "        encode2 = Bidirectional(CuDNNGRU(128, return_sequences=True))\n",
    "        attention = Attention(maxlen)\n",
    "        x_4 = Embedding(len(word_index) + 1,\n",
    "                        embeddings_matrix.shape[1],\n",
    "                        weights=[embeddings_matrix],\n",
    "                        input_length=maxlen,\n",
    "                        trainable=True)(inp)\n",
    "        x_3 = SpatialDropout1D(0.2)(x_4)\n",
    "        x_3 = encode(x_3)\n",
    "        x_3 = Dropout(0.2)(x_3)\n",
    "        x_3 = encode2(x_3)\n",
    "        x_3 = Dropout(0.2)(x_3)\n",
    "        x_3 = Conv1D(64, kernel_size=3, padding=\"valid\", kernel_initializer=\"glorot_uniform\")(x_3)\n",
    "        x_3 = Dropout(0.2)(x_3)\n",
    "        avg_pool_3 = GlobalAveragePooling1D()(x_3)\n",
    "        max_pool_3 = GlobalMaxPooling1D()(x_3)\n",
    "        attention_3 = attention(x_3)\n",
    "        x = keras.layers.concatenate([avg_pool_3, max_pool_3, attention_3])\n",
    "        x = Dense(num_class, activation=\"sigmoid\")(x)\n",
    "\n",
    "        adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "        rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "        model = Model(inputs=inp, outputs=x)\n",
    "        model.compile(\n",
    "            loss='binary_crossentropy',\n",
    "            optimizer=rmsprop\n",
    "            )\n",
    "        return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Train model and performance on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train model using rcnn.  add fast-text pre-trained embedding.\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "#Set the GPU sources.\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "import random\n",
    "random.seed = 42\n",
    "import pandas as pd\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import ModelCheckpoint, Callback\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score\n",
    "from keras.layers import *\n",
    "from classifier_rcnn import TextClassifier\n",
    "from gensim.models.keyedvectors import KeyedVectors    #load word2vec model.\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "# import os\n",
    "# os.environ['OMP_NUM_THREADS'] = '8'  #set the number of threads\n",
    "\n",
    "\n",
    "def getClassification(arr):\n",
    "    arr = list(arr)\n",
    "    res_values = []\n",
    "    for i in range(20):                                            #the 20 classes.\n",
    "        if arr[4 * i: 4 * (i+1)].index(max(arr[4 * i: 4 * (i+1)])) == 0:\n",
    "            res_values.append(-2)\n",
    "        elif arr[4 * i: 4 * (i+1)].index(max(arr[4 * i: 4 * (i+1)])) == 1:\n",
    "            res_values.append(-1)\n",
    "        elif arr[4 * i: 4 * (i+1)].index(max(arr[4 * i: 4 * (i+1)])) == 2:\n",
    "            res_values.append(0)\n",
    "        else:\n",
    "            res_values.append(1)\n",
    "\n",
    "    return res_values\n",
    "\n",
    "class Metrics(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "        self.val_accuracy = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = getClassification(self.model.predict(self.validation_data[0]))\n",
    "        val_targ = getClassification(self.validation_data[1])\n",
    "        #macro_F1: https://blog.csdn.net/sinat_28576553/article/details/80258619.   一般对于多分类任务来讲，macro 要优于 micro.\n",
    "        _val_f1 = f1_score(val_targ, val_predict, average=\"macro\")\n",
    "        _val_recall = recall_score(val_targ, val_predict, average=\"macro\")\n",
    "        _val_precision = precision_score(val_targ, val_predict, average=\"macro\")\n",
    "        _val_accuracy = accuracy_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        self.val_accuracy.append(_val_accuracy)\n",
    "        print(_val_f1, _val_precision, _val_recall, _val_accuracy)\n",
    "        print(\"max f1: {}\".format(max(self.val_f1s)))\n",
    "        print(\"max acc: {}\".format(max(self.val_accuracy)))\n",
    "        return\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"preprocess/train_char.csv\")\n",
    "#eval() 函数用来执行一个字符串表达式，并返回表达式的值。 eg: eval('2+2') -> 4\n",
    "data[\"content\"] = [eval(i) for i in data.content]\n",
    "\n",
    "\n",
    "\n",
    "validation = pd.read_csv(\"preprocess/validation_char.csv\")\n",
    "validation[\"content\"] = [eval(i) for i in validation.content]\n",
    "\n",
    "test = pd.read_csv('preprocess/test_char.csv')\n",
    "test['content'] = [eval(i) for i in test.content]\n",
    "\n",
    "\n",
    "EMBEDDING_FILE = '/media/chenshixin/My Passport/datasets/wiki.zh.vec'\n",
    "\n",
    "model_dir = \"train_rcnn/\"\n",
    "maxlen = 1000  \n",
    "max_features = 20000\n",
    "batch_size = 128     \n",
    "epochs = 20\n",
    "embed_size = 300\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(data.content.values) + list(validation.content.values) + list(test.content.values))   #Tokenizer().fit_on_texts(param:list)\n",
    "X_train = tokenizer.texts_to_sequences(data.content.values)\n",
    "X_valid = tokenizer.texts_to_sequences(validation.content.values)\n",
    "\n",
    "input_train =  sequence.pad_sequences(X_train, maxlen=maxlen)  #Setting the same length of all the comment in train and validation dataset\n",
    "\n",
    "input_validation = sequence.pad_sequences(X_valid, maxlen=maxlen)\n",
    "\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "#Get a dict\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embeddings_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)    #the function of embedding_index.get(word) is get the vector of the word.\n",
    "    if embedding_vector is not None: embeddings_matrix[i] = embedding_vector\n",
    "#for example, word_index:{'the':1}, embedding_index:{'the': vector}, embedding_matrix:{1: vector}\n",
    "\n",
    "#line 86-128 one-hot enconding for all aspects in train and validation dataset\n",
    "\n",
    "Y_train = data.iloc[:, 22:].values\n",
    "Y_validation = validation.iloc[:, 22:].values\n",
    "\n",
    "print(\"model_rcnn\")\n",
    "model = TextClassifier().model(embeddings_matrix, maxlen, word_index, 80)\n",
    "file_path = model_dir + \"model_rcnn_{epoch:02d}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, verbose=1, save_weights_only=True)\n",
    "metrics = Metrics()\n",
    "callbacks_list = [checkpoint, metrics]\n",
    "history = model.fit(input_train, Y_train, batch_size=batch_size, epochs=epochs,\n",
    "                     validation_data=(input_validation, Y_validation), callbacks=callbacks_list, verbose=1)\n",
    "del model1\n",
    "del history\n",
    "gc.collect()       #\n",
    "K.clear_session()\n",
    "\n",
    "#The basic usage of map see: https://www.runoob.com/python/python-func-map.html -> line 40.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q41. 依据Kernel 中选择方法，对数据和代码进行改造，使其符合选择该问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q42. 你现在的模型的准确率是多少？ 如何知道你的模型是不是真的学习了 而不是随机的进行猜测？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q43. 你的模型现在准确度不高的原因，你猜测主要是什么？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q43. 如前文所述，这个问题很难，其实现在也没有什么万灵药方法。 所以需要同学们多想想如何有效， 可以给大家参考的优化方式有， 修改vocabulary size, embedding size,去掉停用词，重新组合词组等。 并且结合使用LSTM， GRU， Bi-RNN， Stacked， Attentional, regularization, 等各种方法组合进行模型的优化， 至少进行10次优化，每次优化请按照以下步骤填写："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回答：\n",
    "\n",
    "---这是一个实例----\n",
    "\n",
    "第1次优化：\n",
    "\n",
    "1. 存在的问题： loss下降太慢；\n",
    "2. 准备进行的优化：减小模型的神经单元数量；\n",
    "3. 期待的结果：loss下降加快；\n",
    "4. 实际结果：loss下降的确加快(或者并没有加快)\n",
    "5. 原因分析：模型神经元数量减小，收敛需要的次数减少，loss下降加快\n",
    "\n",
    "\n",
    "---你的实验优化结构记录在此---\n",
    "\n",
    "**第1次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第2次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第3次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第4次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第5次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第6次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第7次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第9次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n",
    "\n",
    "**第10次优化**：\n",
    "\n",
    "1. 存在的问题： \n",
    "2. 准备进行的优化：\n",
    "3. 期待的结果：\n",
    "4. 实际结果：\n",
    "5. 原因分析：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最后一步： 使用Flask、Bottle、Bootstrap变成一个网络应用并且部署在服务器上，这样别人就可以直接通过网址访问你的应用啦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步，我们使用Bottle，Bootstrap,Flask等工具进行可视化现实，做出网页能够访问的形式，就像我们的第一个项目一样 😁."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次项目的总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请写项目的总结报告，描述此次项目的主要过程，其中遇到的问题，以及如何解决这些问题的，以及有什么经验和收获。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你，你完成了一个**十分**复杂的问题， 能完成这个问题，求是求是，你的能力其实已经达到了国内绝大多数公司的要求，你缺的只是熟练程度。 多多在 Kaggle， 阿里天池里边找一些自己感兴趣的问题，多练习练习。 熟能生巧。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
